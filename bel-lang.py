import chars2vec
import sklearn.decomposition
import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds
from tensorflow.keras.preprocessing.text import Tokenizer

# trainBelDataset = tf.data.TextLineDataset(os.path.join('./resources/', 'train-bel.txt'))
# trainBelDataset.map(lambda string: string)
# trainBelDataset = np.genfromtxt(os.path.join('./resources/', 'train-bel.txt'), dtype=None, usecols=(1, 3), encoding='UTF-8')

trainBelDatasetLines = open(os.path.join('./resources/', 'train-bel.txt'), encoding="utf-8").read().split("\n")
# trainBelDatasetLines = open(os.path.join('./resources/', 'small.txt'), encoding="utf-8").read().split("\n")
belSymbols = frozenset(['–∞','–±','–≤','–≥','–¥','–µ','—ë','–∂','–∑','—ñ','–π','–∫','–ª','–º','–Ω','–æ','–ø','—Ä','—Å','—Ç','—É','—û','—Ñ','—Ö','—Ü','—á','—à','—ã','—å','—ç','—é', '—è', '‚Äô'])

# tokenizer = Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n‚Äî‚Ä¶‚Äì‚Äë0123456789¬´¬ªabcdefghijklmnopqrstuvwxyz‚Äû‚Äü‚Äô‚Äú¬¨‚Äù¬∞·É§·Éî√°≈ô—çÃÅ·Éô·Éù', lower=True,
tokenizer = Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n‚Äî‚Ä¶‚Äì‚Äë0123456789¬´¬ªabcdefghijklmnopqrstuvwxyz·Éê¬©¬ß‚Äπ…ôƒô·ΩêŒØ¬∂œÅ\x98¬µ¬Æ·É†√≠¬≥""œÖ≈º¬∑·Éò¬∫≈ô¬±¬¥≈≠·ºÄ‚Äù·ÉôŒπ\uf0fc‚Ä∞·ÉîÊÑè‚Ä∫\u200b‰Ω†—ò√ΩÀö\u202c√©‚Ä°Œ±√ü¬¨œÇÃÅ√°\ufeffƒç¬¶—ü—ì‚ÜêŒ≠œâ\uf020‚Ä†‚Äû‚Ç¨ å—ö—í\u200e‚Äö—ï‚àô—ô ä·É§¬≤œÄ·Éù“∑¬∞‚Äú√´≈° ºƒº\u202aÁöÑÃìŒΩ‚àí‚Ä¢‚Ñ¢Œµ—óƒì√ºœå É—î‚ÑñƒáÔøΩüåøŒ∑\uf008·ÉõÀàÂêå\x03—õŒø—ú√øÊàë≈Ç“ëÊñº√≥œéÃÜ≈æÂ∞ç‚ÄòƒÖ≈õ¬§—£Õæ√ó\'', lower=True,
    split=' ', char_level=False
)
# sequences = tokenizer.texts_to_sequences(trainBelDataset)
# print(sequences)
tokenizer.word_index
tokenizer.fit_on_texts(trainBelDatasetLines)
sequences = tokenizer.texts_to_sequences(trainBelDatasetLines)

wordsBeginnings = set()
wordsEndings = set()
for word in tokenizer.word_index:
    if len(word) > 2:
      wordsBeginnings.add(word[ 0 : 3 ])
      wordsEndings.add(word[-3:])

blackListChars = set()

with open(os.path.join('./resources/', 'endings-3.txt'), 'w', encoding="utf-8") as f:
    for word in wordsEndings:
        f.write("%s\n" % word)

with open(os.path.join('./resources/', 'beginnings-3.txt'), 'w', encoding="utf-8") as f:
    for word in wordsBeginnings:
        f.write("%s\n" % word)
        # for char in word:
        #     if not char in belSymbols:
        #       blackListChars.add(char)

print(blackListChars)

# wordsBeginnings = {x for x in wordsBeginnings if len(x) > 1}
# wordsEndings = {x for x in wordsEndings if len(x) > 1}


# Load Inutition Engineering pretrained model
# Models names: 'eng_50', 'eng_100', 'eng_150' 'eng_200', 'eng_300'
# c2v_model = chars2vec.load_model('eng_50')

# words = [
#   'NaturalLanguage',
#   'NaturalLanguageAndSomethingElse',
#   'NaturalLangu',

#   'NaturalUnderstanding',
#   'NaturalUnderstandingAnd',
#   'NaturalUnderstandingAndBla',
#   'NaturalUnderstandingAndBlaBla',
#   'NaturalUnderstandingAndSomethingElse',
# ]

# # Create word embeddings
# word_embeddings = c2v_model.vectorize_words(words)

# # Project embeddings on plane using the PCA
# projection_2d = sklearn.decomposition.PCA(n_components=2).fit_transform(word_embeddings)

# # Draw words on plane
# f = plt.figure(figsize=(8, 6))

# for j in range(len(projection_2d)):
#     plt.scatter(projection_2d[j, 0], projection_2d[j, 1],
#                 marker=('$' + words[j] + '$'),
#                 s=500 * len(words[j]), label=j,
#                 facecolors='green' if words[j] 
#                            in ['Natural', 'Language', 'Understanding'] else 'black')
# plt.show()